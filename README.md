This neuro heuristics project is a part of a larger passion project where I map human cognitive traits onto machine behaviour. I wanted to explore how large langauge models exhibit cognitive biases like anchoring, satisficing, and recency.
Heuristics used:
1. Anchoring - Does a strong numerical or semantic anchor in the prompt skew the model's prompt?
2. Satisficing - Will the model settle for a "good enough" answer if the prompt implies speed or a cognitive constraint?
3. Recency - Does providing recent information influence the model's prediction even when older information is more relevant?

4. 
